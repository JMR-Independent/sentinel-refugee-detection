{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Validation Analysis\n",
    "\n",
    "Additional validation of detected candidates:\n",
    "1. Cross-reference with OSM/UNHCR databases\n",
    "2. Distance to nearest road\n",
    "3. Temporal analysis (did the settlement appear recently?)\n",
    "4. Generate all paper figures\n",
    "\n",
    "**Input:** Detection results from notebook 05, trained model  \n",
    "**Output:** Validation metrics, paper-ready figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab setup (uncomment if running on Colab) ---\n",
    "# PROJECT_DIR = '/content/drive/MyDrive/sentinel-refugee-detection'\n",
    "\n",
    "# --- Local setup ---\n",
    "PROJECT_DIR = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import load_config, _haversine_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(f'{PROJECT_DIR}/configs/default.yaml')\n",
    "\n",
    "# Load locations with predictions (saved from notebook 05)\n",
    "locations = pd.read_csv(f'{PROJECT_DIR}/data/labels/all_locations.csv')\n",
    "# You'll need to add prediction columns from notebook 05\n",
    "# For now, this is a template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross-Reference with Known Databases\n",
    "\n",
    "For each high-confidence detection, check if it matches a known camp in OSM or UNHCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_reference_detections(detections_df, known_camps_df, radius_km=2.0):\n",
    "    \"\"\"Check if detections match known camps within a radius.\n",
    "    \n",
    "    Returns detections_df with added columns:\n",
    "    - nearest_known_km: distance to nearest known camp\n",
    "    - matched_known: True if within radius_km of a known camp\n",
    "    - nearest_known_name: name of nearest known camp\n",
    "    \"\"\"\n",
    "    known_coords = known_camps_df[['lat', 'lon']].values\n",
    "    known_names = known_camps_df['name'].values\n",
    "    \n",
    "    nearest_dists = []\n",
    "    nearest_names = []\n",
    "    \n",
    "    for _, row in detections_df.iterrows():\n",
    "        dists = [_haversine_km(row['lat'], row['lon'], kc[0], kc[1]) \n",
    "                 for kc in known_coords]\n",
    "        if dists:\n",
    "            min_idx = np.argmin(dists)\n",
    "            nearest_dists.append(dists[min_idx])\n",
    "            nearest_names.append(known_names[min_idx])\n",
    "        else:\n",
    "            nearest_dists.append(np.inf)\n",
    "            nearest_names.append('none')\n",
    "    \n",
    "    detections_df = detections_df.copy()\n",
    "    detections_df['nearest_known_km'] = nearest_dists\n",
    "    detections_df['matched_known'] = [d < radius_km for d in nearest_dists]\n",
    "    detections_df['nearest_known_name'] = nearest_names\n",
    "    \n",
    "    return detections_df\n",
    "\n",
    "# Example usage (will work once you have predictions from notebook 05):\n",
    "# high_conf_detections = predictions[predictions['prob'] >= 0.8]\n",
    "# known_camps = locations[locations['source'].isin(['osm', 'unhcr'])]\n",
    "# validated = cross_reference_detections(high_conf_detections, known_camps)\n",
    "# \n",
    "# n_new = (~validated['matched_known']).sum()\n",
    "# print(f\"Candidate NEW detections (not in any database): {n_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distance to Nearest Road (from OSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_nearest_road_distance(lat, lon, radius_m=5000):\n",
    "    \"\"\"Query Overpass API for nearest road and compute distance.\"\"\"\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:30];\n",
    "    way[\"highway\"](around:{radius_m},{lat},{lon});\n",
    "    out center 1;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            'https://overpass-api.de/api/interpreter',\n",
    "            params={'data': query}, timeout=60,\n",
    "        )\n",
    "        data = resp.json()\n",
    "        \n",
    "        if data.get('elements'):\n",
    "            road = data['elements'][0]\n",
    "            if 'center' in road:\n",
    "                return _haversine_km(\n",
    "                    lat, lon,\n",
    "                    road['center']['lat'], road['center']['lon'],\n",
    "                )\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return None  # No road found within radius\n",
    "\n",
    "# Example: compute road distances for all high-confidence detections\n",
    "# import time\n",
    "# road_dists = []\n",
    "# for _, row in high_conf_detections.iterrows():\n",
    "#     d = get_nearest_road_distance(row['lat'], row['lon'])\n",
    "#     road_dists.append(d)\n",
    "#     time.sleep(1)  # Rate limiting\n",
    "# high_conf_detections['road_dist_km'] = road_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Paper Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Study area map with train/test country split\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "train_locs = locations[locations['country'].isin(config['train_countries'])]\n",
    "test_locs = locations[locations['country'].isin(config['test_countries'])]\n",
    "\n",
    "camps = locations[locations['label'] == 'camp']\n",
    "negs = locations[locations['label'] == 'non-camp']\n",
    "\n",
    "ax.scatter(negs['lon'], negs['lat'], c='lightgray', s=5, alpha=0.3, label='Negative', zorder=1)\n",
    "\n",
    "train_camps = camps[camps['country'].isin(config['train_countries'])]\n",
    "test_camps = camps[camps['country'].isin(config['test_countries'])]\n",
    "\n",
    "ax.scatter(train_camps['lon'], train_camps['lat'], c='blue', s=30, \n",
    "           alpha=0.8, label='Train camps', zorder=2)\n",
    "ax.scatter(test_camps['lon'], test_camps['lat'], c='red', s=30,\n",
    "           alpha=0.8, label='Test camps', zorder=2)\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Study Area: Train (blue) vs Test (red) Countries')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/paper/fig1_study_area.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Method overview (placeholder - create in drawing tool)\n",
    "print(\"Figure 2: Create method overview diagram externally\")\n",
    "print(\"  Sentinel-2 tile -> 6-band crop -> ResNet-18 -> camp/non-camp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Model comparison bar chart (template)\n",
    "# Fill in actual values after training\n",
    "\n",
    "models = ['NDVI Threshold', 'Random Forest', 'ResNet-18']\n",
    "# Replace with actual values:\n",
    "precision_vals = [0.0, 0.0, 0.0]  # TODO\n",
    "recall_vals = [0.0, 0.0, 0.0]     # TODO\n",
    "f1_vals = [0.0, 0.0, 0.0]         # TODO\n",
    "auc_vals = [0.0, 0.0, 0.0]        # TODO\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(x - 1.5*width, precision_vals, width, label='Precision', color='#2196F3')\n",
    "ax.bar(x - 0.5*width, recall_vals, width, label='Recall', color='#4CAF50')\n",
    "ax.bar(x + 0.5*width, f1_vals, width, label='F1', color='#FF9800')\n",
    "ax.bar(x + 1.5*width, auc_vals, width, label='AUC', color='#9C27B0')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison (Test Set - Unseen Countries)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/paper/fig3_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY FOR PAPER\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\")\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Total locations: {len(locations)}\")\n",
    "print(f\"  Camps: {(locations['label'] == 'camp').sum()}\")\n",
    "print(f\"  Negatives: {(locations['label'] == 'non-camp').sum()}\")\n",
    "print(f\"  Train countries: {config['train_countries']}\")\n",
    "print(f\"  Test countries: {config['test_countries']}\")\n",
    "print(f\"\")\n",
    "print(f\"Sentinel-2 parameters:\")\n",
    "print(f\"  Bands: {config['bands']}\")\n",
    "print(f\"  Tile size: {config['tile_size']}px ({config['tile_size']*config['resolution']}m)\")\n",
    "print(f\"  Date range: {config['date_range']}\")\n",
    "print(f\"\")\n",
    "print(f\"Fill in after training:\")\n",
    "print(f\"  ResNet-18 test precision: ___\")\n",
    "print(f\"  ResNet-18 test recall: ___\")\n",
    "print(f\"  ResNet-18 test AUC: ___\")\n",
    "print(f\"  Candidate new detections: ___\")\n",
    "print(f\"  Detections not in OSM: ___\")\n",
    "print(f\"  Detections not in UNHCR: ___\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
