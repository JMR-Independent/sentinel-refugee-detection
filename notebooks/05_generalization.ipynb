{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Generalization Testing\n",
    "\n",
    "Tests the trained model on **unseen countries** (Chad, Ethiopia, Yemen).\n",
    "\n",
    "**Key metrics:**\n",
    "- Standard: Precision, Recall, F1, ROC-AUC\n",
    "- **Precision@Top-K**: Of our top 50 detections, how many are real? (operationally useful)\n",
    "- **Error analysis by negative category**: Where does the model fail?\n",
    "\n",
    "**Input:** `checkpoints/best_model.pth`, `data/tiles/manifest.csv`  \n",
    "**Output:** Generalization metrics, candidate new detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab setup (uncomment if running on Colab) ---\n",
    "# PROJECT_DIR = '/content/drive/MyDrive/sentinel-refugee-detection'\n",
    "\n",
    "# --- Local setup ---\n",
    "PROJECT_DIR = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, PROJECT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.utils import load_config\n",
    "from src.data import CampTileDataset\n",
    "from src.model import create_camp_classifier\n",
    "from src.train import evaluate, predict, precision_at_top_k\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(f'{PROJECT_DIR}/configs/default.yaml')\n",
    "tiles_dir = Path(f'{PROJECT_DIR}/data/tiles')\n",
    "\n",
    "stats = np.load(tiles_dir / 'norm_stats.npz')\n",
    "norm_stats = {'low': stats['low'], 'high': stats['high']}\n",
    "\n",
    "model = create_camp_classifier(config)\n",
    "model.load_state_dict(\n",
    "    torch.load(f'{PROJECT_DIR}/checkpoints/best_model.pth', map_location=device)\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Set Evaluation (Unseen Countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dataset = CampTileDataset(\n",
    "    manifest_path=tiles_dir / 'manifest.csv',\n",
    "    split='test',\n",
    "    transform=None,\n",
    "    normalize=True,\n",
    "    norm_stats=norm_stats,\n",
    "    model_size=config['tile_size_model'],\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "pos_weight_cfg = config['model'].get('pos_weight', 'auto')\n",
    "if pos_weight_cfg in (None, 'auto'):\n",
    "    manifest = pd.read_csv(tiles_dir / 'manifest.csv')\n",
    "    train_labels = manifest[manifest['split'] == 'train']['label']\n",
    "    n_pos = train_labels.isin(['camp', 'camp_context']).sum()\n",
    "    n_neg = (~train_labels.isin(['camp', 'camp_context'])).sum()\n",
    "    pos_weight_value = (n_neg / max(n_pos, 1)) if n_pos > 0 else 1.0\n",
    "else:\n",
    "    pos_weight_value = float(pos_weight_cfg)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor([pos_weight_value], device=device)\n",
    ")\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print('\n",
    "' + '='*50)\n",
    "print('TEST SET RESULTS (Unseen Countries)')\n",
    "print('='*50)\n",
    "print(f'Precision: {test_metrics[\"precision\"]:.3f}')\n",
    "print(f'Recall:    {test_metrics[\"recall\"]:.3f}')\n",
    "print(f'F1:        {test_metrics[\"f1\"]:.3f}')\n",
    "print(f'ROC-AUC:   {test_metrics[\"auc\"]:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision @ Top-K\n",
    "\n",
    "The operationally useful metric: if we send a team to investigate the top 50 detections, how many are real camps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_metrics['labels']\n",
    "probs = test_metrics['probs']\n",
    "\n",
    "for k in [10, 20, 50]:\n",
    "    p_at_k, details = precision_at_top_k(labels, probs, k=k)\n",
    "    print(f'Precision@{k:>3d}: {p_at_k:.3f} '\n",
    "          f'({details[\"true_positives\"]} TP, {details[\"false_positives\"]} FP, '\n",
    "          f'min_prob={details[\"min_prob\"]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Country Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "test_manifest = test_dataset.manifest\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(f\"{'Country':<15} {'N':>5} {'Prec':>8} {'Rec':>8} {'F1':>8} {'AUC':>8}\")\n",
    "print('-' * 55)\n",
    "\n",
    "for country in config['test_countries']:\n",
    "    mask = test_manifest['country'].values == country\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    c_labels = labels[mask]\n",
    "    c_probs = probs[mask]\n",
    "    c_preds = preds[mask]\n",
    "    \n",
    "    p = precision_score(c_labels, c_preds, zero_division=0)\n",
    "    r = recall_score(c_labels, c_preds, zero_division=0)\n",
    "    f = f1_score(c_labels, c_preds, zero_division=0)\n",
    "    auc = roc_auc_score(c_labels, c_probs) if len(np.unique(c_labels)) > 1 else 0\n",
    "    \n",
    "    print(f'{country:<15} {mask.sum():>5} {p:>8.3f} {r:>8.3f} {f:>8.3f} {auc:>8.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Analysis by Negative Category\n",
    "\n",
    "Where does the model make false positives? Rural? Urban? Barren?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive analysis by negative category\n",
    "neg_mask = labels == 0\n",
    "neg_preds = preds[neg_mask]\n",
    "neg_categories = test_manifest['neg_category'].values[neg_mask]\n",
    "\n",
    "print('False Positive Rate by Negative Category:')\n",
    "print(f\"{'Category':<15} {'N':>6} {'FP':>6} {'FPR':>8}\")\n",
    "print('-' * 40)\n",
    "\n",
    "for cat in ['rural', 'urban', 'barren', '']:\n",
    "    cat_mask = neg_categories == cat\n",
    "    if cat_mask.sum() == 0:\n",
    "        continue\n",
    "    cat_preds = neg_preds[cat_mask]\n",
    "    fp = cat_preds.sum()\n",
    "    fpr = fp / cat_mask.sum()\n",
    "    cat_name = cat if cat else 'camp_peripheral'\n",
    "    print(f'{cat_name:<15} {cat_mask.sum():>6} {fp:>6} {fpr:>8.3f}')\n",
    "\n",
    "print('\\nKey insight: high FPR on urban = model confuses dense formal settlements with camps')\n",
    "print('             high FPR on barren = model triggers on bare soil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. High-Confidence Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = config['inference']['confidence_threshold']\n",
    "\n",
    "test_manifest = test_manifest.copy()\n",
    "test_manifest['prob'] = probs\n",
    "test_manifest['predicted'] = preds\n",
    "\n",
    "high_conf = test_manifest[test_manifest['prob'] >= threshold]\n",
    "print(f'\\nHigh-confidence detections (p >= {threshold}): {len(high_conf)}')\n",
    "print(f'  True camps: {(high_conf[\"label\"] == \"camp\").sum()}')\n",
    "print(f'  False positives: {(high_conf[\"label\"] != \"camp\").sum()}')\n",
    "\n",
    "if len(high_conf) > 0:\n",
    "    print('\\nTop 20 detections:')\n",
    "    print(high_conf[['tile_id', 'country', 'label', 'neg_category', 'prob']]\n",
    "          .sort_values('prob', ascending=False)\n",
    "          .head(20)\n",
    "          .to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(labels, probs)\n",
    "axes[0].plot(fpr, tpr, 'b-', label=f'AUC={test_metrics[\"auc\"]:.3f}')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Test Set - Unseen Countries)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "ConfusionMatrixDisplay(cm, display_labels=['non-camp', 'camp']).plot(ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "# Score distribution\n",
    "axes[2].hist(probs[labels == 0], bins=20, alpha=0.5, label='non-camp', color='blue')\n",
    "axes[2].hist(probs[labels == 1], bins=20, alpha=0.5, label='camp', color='red')\n",
    "axes[2].axvline(0.5, color='k', linestyle='--', label='threshold')\n",
    "axes[2].set_xlabel('Predicted Probability')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Score Distribution')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/generalization_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tiles: TP, FP, TN, FN\n",
    "categories = {\n",
    "    'True Positive': (labels == 1) & (preds == 1),\n",
    "    'False Positive': (labels == 0) & (preds == 1),\n",
    "    'True Negative': (labels == 0) & (preds == 0),\n",
    "    'False Negative': (labels == 1) & (preds == 0),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "\n",
    "for row_idx, (cat_name, mask) in enumerate(categories.items()):\n",
    "    indices = np.where(mask)[0][:3]\n",
    "    for col_idx, idx in enumerate(indices):\n",
    "        tile = np.load(test_manifest.iloc[idx]['path'])\n",
    "        # Channels: R, G, B, NDVI, NDBI, SWIR_ratio\n",
    "        rgb = tile[:3].transpose(1, 2, 0)\n",
    "        rgb = np.clip(rgb / np.percentile(rgb, 98), 0, 1)\n",
    "        axes[row_idx, col_idx].imshow(rgb)\n",
    "        info = test_manifest.iloc[idx]\n",
    "        axes[row_idx, col_idx].set_title(\n",
    "            f\"{info['tile_id']}\\np={probs[idx]:.2f} | {info.get('neg_category', '')}\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "    axes[row_idx, 0].set_ylabel(cat_name, fontsize=11)\n",
    "\n",
    "plt.suptitle('Test Set Examples (Unseen Countries)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/example_tiles.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}